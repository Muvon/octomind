# Octomind Configuration File
# This file contains all configurable settings for Octomind.
# All values shown here are the defaults - you can customize any of them.
#
# 💡 Tips:
#   • View current config: octomind config --show
#   • Validate config: octomind config --validate

# Configuration version (DO NOT MODIFY - used for automatic upgrades)
version = 1

# ═══════════════════════════════════════════════════════════════════════════════
# SYSTEM-WIDE SETTINGS
# These settings apply globally across all roles and commands
# ═══════════════════════════════════════════════════════════════════════════════

# Log level for system messages (none, info, debug)
# • none: No logging output (cleanest experience)
# • info: Show important operations and status messages
# • debug: Show detailed debugging information
log_level = "none"

# Default model for all operations (provider:model format)
# This is the fallback model when role-specific models aren't specified
# Examples: "openrouter:anthropic/claude-3.5-sonnet", "openai:gpt-4o"
model = "openrouter:anthropic/claude-sonnet-4"

# ═══════════════════════════════════════════════════════════════════════════════
# PERFORMANCE & LIMITS
# Configure thresholds and performance-related settings
# ═══════════════════════════════════════════════════════════════════════════════

# Warn when MCP tool responses exceed this token count (0 = disable warnings)
mcp_response_warning_threshold = 20000

# Maximum tokens per request before auto-truncation kicks in (0 = no limit)
max_request_tokens_threshold = 20000

# Enable automatic truncation of large inputs to fit within token limits
enable_auto_truncation = false

# Cache responses when they exceed this token count (0 = no caching)
cache_tokens_threshold = 2048

# How long to keep cached responses (in seconds)
cache_timeout_seconds = 240

# Wether to use long system cache (longer cache lifetime)
use_long_system_cache = true

# ═══════════════════════════════════════════════════════════════════════════════
# AGENT CONFIGURATIONS
# Define specific AI agents that route tasks to configured layers
# Each agent becomes a separate MCP tool (e.g., agent_code_reviewer, agent_debugger)
# name = name in defined layers
# ═══════════════════════════════════════════════════════════════════════════════

# [[agents]]
# name = "code_reviewer"
# description = "Review code for performance, security, and best practices issues. Analyzes code quality and suggests improvements."

# [[agents]]
# name = "debugger"
# description = "Analyze bugs, trace issues, and suggest debugging approaches. Helps identify root causes and solutions."

# [[agents]]
# name = "architect"
# description = "Design system architecture and evaluate technical decisions. Provides high-level design guidance."

# ═══════════════════════════════════════════════════════════════════════════════
# USER INTERFACE
# Configure how Octomind displays information
# ═══════════════════════════════════════════════════════════════════════════════

# Enable markdown rendering for AI responses (makes output prettier)
enable_markdown_rendering = true

# Markdown theme for styling (default, dark, light, ocean, solarized, monokai)
# Use 'octomind config --list-themes' to see all available themes
markdown_theme = "default"

# Session spending threshold in USD (0.0 = no limit)
# When exceeded, Octomind will prompt before continuing
max_session_spending_threshold = 0.0

# ═══════════════════════════════════════════════════════════════════════════════
# API KEYS AND AUTHENTICATION
# All API keys are read from environment variables for security
# Set these environment variables before running Octomind:
#   • OPENROUTER_API_KEY - for OpenRouter (https://openrouter.ai/)
#   • OPENAI_API_KEY - for OpenAI (https://platform.openai.com/)
#   • ANTHROPIC_API_KEY - for Anthropic (https://console.anthropic.com/)
#   • GOOGLE_APPLICATION_CREDENTIALS - path to Google Cloud credentials JSON
#   • AWS_ACCESS_KEY_ID - for Amazon Bedrock
#   • CLOUDFLARE_API_TOKEN - for Cloudflare Workers AI
# ═══════════════════════════════════════════════════════════════════════════════

# ═══════════════════════════════════════════════════════════════════════════════
# ROLE CONFIGURATIONS
# Configure behavior for different roles using [[roles]] array format
# ═══════════════════════════════════════════════════════════════════════════════

# Developer role - optimized for coding and development tasks
[[roles]]
name = "developer"
# Enable layers system for complex multi-step operations
enable_layers = true
# Temperature for AI responses (0.0 to 1.0)
temperature = 0.2

# Layer references for developer role (empty = no layers enabled)
layer_refs = ["query_processor", "context_generator"]

# System prompt for developer role (uses built-in developer prompt if not specified)
# Default developer system prompt:
system = """You are an Octomind – top notch fully autonomous AI developer that act with best pracises like a Senior level developer.
Current working dir: %{CWD}

**DEVELOPMENT APPROACH:**
1. Analyze problems thoroughly first by thinking through solution step-by-step
2. When you missing context or lack of understanding, try to use your memories first
3. If you really NOT sure what to do or not enough information to proceed, you may ask for help
4. Process with most efficient approach possible and execute necessary changes directly using available tools
5. Think and act like a professional highly skilled developer of the Senior level

**CODE QUALITY GUIDELINES:**
• Provide validated, working solutions
• Keep code clear, concise and maintainable
• Focus on practical solutions and industry best practices
• Avoid unnecessary abstractions - solve problems directly
• Don't over-fragment code across very small multiple files

**MISSING CONTEXT COLLECTION CHECKLIST:**
1. Read your memories first if you missing context or lack of understanding
2. Examine key project files to understand the codebase structure
3. Use view_signatures when you just need to get method/functions from files to get initial understanding
4. Use text_editor view to examine files and read it full contents when you really need it, prefer to use view_range
5. If needed, use semantic_search tools and list_files to find relevant implementation patterns
6. Remember – reading all files with their complete contents should be considered a last resort, only when you absolutely need to know the FULL contents of a file.

**WHEN WORKING WITH FILES:**
1. First, understand which files you need to read/write and most important what parts of it due to files may be large
2. Process files efficiently, preferably in a single operation to avoid many tokens return
3. For multiple file modifications, use batch_edit command to perform all changes in one operation
4. Utilize the provided tools proactively without asking if you should use them

%{SYSTEM}

%{README}

IMPORTANT:
- Right now you are *NOT* in the chat only mode and have access to tool use and system.
- Do not sidetrack and *FOCUS* specifically on requested task to be completed
- Make sure when you refactor code or do changes, you do not remove critical parts of the codebase."""

# MCP configuration for developer role
mcp = { server_refs = ["developer", "filesystem", "octocode"], allowed_tools = [] }

# Assistant role - optimized for general assistance tasks
[[roles]]
name = "assistant"
enable_layers = false
temperature = 0.7
layer_refs = []
system = "You are a helpful assistant."
# MCP configuration for assistant role
mcp = { server_refs = ["filesystem"], allowed_tools = [] }

# ═══════════════════════════════════════════════════════════════════════════════
# MCP (MODEL CONTEXT PROTOCOL) SERVERS
# Configure external MCP servers and tools
# Built-in servers are defined here for transparency and easy customization
# ═══════════════════════════════════════════════════════════════════════════════

[mcp]
# Global tool restrictions (empty = no restrictions)
allowed_tools = []

# Built-in MCP servers (always available)
[[mcp.servers]]
name = "developer"
server_type = "developer"
timeout_seconds = 30
args = []
mode = "http"
tools = []

[[mcp.servers]]
name = "agent"
server_type = "agent"
timeout_seconds = 30
args = []
mode = "http"
tools = []

[[mcp.servers]]
name = "filesystem"
server_type = "filesystem"
timeout_seconds = 30
args = []
mode = "http"
tools = []

[[mcp.servers]]
name = "octocode"
server_type = "external"
command = "octocode"
args = ["mcp", "--path=."]
mode = "stdin"
timeout_seconds = 240
tools = []
builtin = true

# Example external MCP server configuration:
# [[mcp.servers]]
# name = "my_custom_server"
# server_type = "external"
# url = "http://localhost:3000/mcp"
# mode = "http"
# timeout_seconds = 30
# auth_token = "optional-auth-token"
# tools = []
# builtin = false

# ═══════════════════════════════════════════════════════════════════════════════
# LAYERS (AI PROCESSING PIPELINE)
# Configure AI processing layers and pipelines
# Built-in layers are defined here for transparency and easy customization
# ═══════════════════════════════════════════════════════════════════════════════

# Built-in core layers (always available)
[[layers]]
name = "query_processor"
# Default query_processor system prompt:
system_prompt = """You are an expert query processor and requirement analyst in the Octomind system. Your task is to analyze user requests and transform them into clearer, more actionable forms.

Given a user request:
1. Identify the core requirement and intent
2. Structure and refine the request while preserving its fundamental purpose
3. Clarify ambiguities and add helpful technical specifics
4. Format the output as well-structured development tasks/requirements
5. Include relevant edge cases, constraints, and success criteria

Guidelines:
- Make minimal changes if the request is already clear and specific
- Return the original text if the request cannot be understood or lacks sufficient context
- Never invent features or requirements not implied in the original request
- Focus solely on requirement analysis - do not implement solutions or write code
- Return only the refined task description without adding commentary
- If ambiguous or incomplete, preserve the original request rather than making assumptions

%{CONTEXT}"""
model = "openrouter:openai/gpt-4.1-mini"
temperature = 0.2
input_mode = "Last"
builtin = true

[layers.mcp]
server_refs = []
allowed_tools = []

[layers.parameters]

[[layers]]
name = "context_generator"
# Default context_generator system prompt:
system_prompt = """You are a context gathering specialist for development tasks.

When given a new task, help me understand what I need to know before implementing it by:

- First: Look into file signatures with semantic_code tool and try to analyze project structure related to task
- Then: If needed, use list_files to find relevant implementation patterns
- If needed: Use text_editor view to examine files and understand interfaces and code signatures
- Only when necessary: Look at detailed implementations

For each task type, focus on different aspects:
- Configuration tasks: Config files, env settings, build scripts
- Feature implementation: Related modules, interfaces, patterns
- Bug fixes: Affected components and dependencies
- Refactoring: Impacted modules and relationships

Provide a clear summary with:
- Core task requirements decomposed the way you are project manager who made it
- Recommendations to look into list of given fields needing examination (with reasons)
- Key code structures and patterns found
- Potential implementation challenges
- Areas where more information might help

Your goal is helping me fully understand what's needed to implement the task successfully.

%{SYSTEM}

%{CONTEXT}"""
model = "openrouter:google/gemini-2.5-flash-preview"
temperature = 0.2
input_mode = "Last"
builtin = true

[layers.mcp]
server_refs = ["developer", "filesystem"]
allowed_tools = ["text_editor", "list_files"]

[layers.parameters]

[[layers]]
name = "reducer"
# Default reducer system prompt:
system_prompt = """You are the session optimizer for Octomind, responsible for consolidating information and preparing for the next interaction.

Your responsibilities:
1. Review the original request and the developer's solution
2. Ensure documentation (README.md and CHANGES.md) is properly updated
3. Create a concise summary of the work that was done
4. Condense the context in a way that preserves essential information for future requests

This condensed information will be cached to reduce token usage in the next iteration.
Focus on extracting the most important technical details while removing unnecessary verbosity.
Your output will be used as context for the next user interaction, so it must contain all essential information
while being as concise as possible.

%{CONTEXT}"""
model = "openrouter:openai/o4-mini"
temperature = 0.2
input_mode = "All"
builtin = true

[layers.mcp]
server_refs = []
allowed_tools = []

[layers.parameters]

# ═══════════════════════════════════════════════════════════════════════════════
# ADVANCED CONFIGURATION
# These sections are for advanced users and custom setups
# Most users won't need to modify these
# ═══════════════════════════════════════════════════════════════════════════════

# Example custom layer configuration:
# [[layers]]
# name = "analysis"
# model = "openrouter:anthropic/claude-3.5-sonnet"
# system_prompt = "You are an expert analyst."
# temperature = 0.3
# input_mode = "Last"
# builtin = false
#
# [layers.mcp]
# server_refs = ["developer", "filesystem"]
# allowed_tools = []
#
# [layers.parameters]
# analysis_type = "detailed"

# Example agent layers (use with agent tool):
# [[layers]]
# name = "code_reviewer"
# model = "openrouter:anthropic/claude-3.5-sonnet"
# system_prompt = "You are a senior code reviewer. Analyze code for quality, performance, security, and best practices. Provide detailed feedback with specific suggestions for improvement."
# temperature = 0.1
# input_mode = "Last"
# builtin = false
#
# [layers.mcp]
# server_refs = ["developer", "filesystem"]
# allowed_tools = ["text_editor", "list_files"]
#
# [[layers]]
# name = "docs_writer"
# model = "openrouter:openai/gpt-4o"
# system_prompt = "You are a technical documentation specialist. Write clear, comprehensive documentation that is easy to understand and follow. Include examples and best practices."
# temperature = 0.2
# input_mode = "Last"
# builtin = false
#
# [layers.mcp]
# server_refs = ["filesystem"]
# allowed_tools = ["text_editor", "list_files"]
#
# [[layers]]
# name = "bug_hunter"
# model = "openrouter:anthropic/claude-3.5-sonnet"
# system_prompt = "You are an expert bug hunter and debugger. Analyze code and logs to identify issues, trace problems to their root cause, and suggest fixes."
# temperature = 0.1
# input_mode = "Last"
# builtin = false
#
# [layers.mcp]
# server_refs = ["developer", "filesystem"]
# allowed_tools = []
#
# Usage examples:
# - agent(name="code_reviewer", task="Review this function for performance issues")
# - agent(name="docs_writer", task="Write documentation for this API endpoint")
# - agent(name="bug_hunter", task="Help me debug this error message")

# Example custom command configuration:
# [[commands]]
# name = "estimate"
# model = "openrouter:openai/gpt-4o-mini"
# system_prompt = "You are a project estimation expert."
# temperature = 0.2
# input_mode = "Last"
#
# [commands.mcp]
# server_refs = []
# allowed_tools = []
#
# [commands.parameters]

# Global system prompt override (uncomment to set a global default)
# system = "You are Octomind, an intelligent AI assistant."
